{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/models/tree/master/official/resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import functools\n",
    "\n",
    "import jax.numpy as np\n",
    "import numpy.random as npr\n",
    "import tensorflow as tf\n",
    "\n",
    "from jax import jit, grad, random, lax\n",
    "from jax.experimental import optimizers, stax\n",
    "from jax.experimental.stax import (AvgPool, BatchNorm, Conv, Dense, FanInSum,\n",
    "                                   FanOut, Flatten, GeneralConv, Identity,\n",
    "                                   MaxPool, Relu, LogSoftmax, Dropout)\n",
    "from tqdm import tqdm_notebook\n",
    "from utils import get_ds_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "step_size = 1e-3\n",
    "data_dir = '/projects/tfds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_layer(**fun_kwargs):\n",
    "  pad_size = np.sum(fun_kwargs.get('padding_config'), axis=1)\n",
    "  init_fun = lambda rng, input_shape: (tuple(np.sum((input_shape, pad_size), axis=2)), ())\n",
    "  apply_fun = lambda params, inputs, **kwargs: lax.pad(inputs, **fun_kwargs)\n",
    "  return init_fun, apply_fun\n",
    "\n",
    "\n",
    "def ConvBlock(kernel_size, filters, strides=(2, 2)):\n",
    "  ks = kernel_size\n",
    "  filters1, filters2 = filters\n",
    "  Main = stax.serial(\n",
    "      Conv(filters1, (ks, ks), strides, padding='SAME'),\n",
    "      BatchNorm(), Relu,\n",
    "      Conv(filters2, (ks, ks), padding='SAME'),\n",
    "      BatchNorm())\n",
    "  Shortcut = stax.serial(\n",
    "      Conv(filters2, (1, 1), strides, W_init=None),\n",
    "      BatchNorm())\n",
    "  return stax.serial(FanOut(2), stax.parallel(Main, Shortcut), FanInSum, Relu)\n",
    "\n",
    "\n",
    "def IdentityBlock(kernel_size, filters):\n",
    "  ks = kernel_size\n",
    "  filters1, filters2 = filters\n",
    "  def make_main(input_shape):\n",
    "    return stax.serial(\n",
    "        Conv(filters1, (1, 1), padding='SAME'),\n",
    "        BatchNorm(), Relu,\n",
    "        Conv(filters2, (ks, ks), padding='SAME'),\n",
    "        BatchNorm())\n",
    "  Main = stax.shape_dependent(make_main)\n",
    "  return stax.serial(FanOut(2), stax.parallel(Main, Identity), FanInSum, Relu)\n",
    "\n",
    "\n",
    "def ResNet20(num_classes):\n",
    "  return stax.serial(\n",
    "      pad_layer(padding_value=0.0, padding_config=((0, 0, 0), (1, 1, 0), (1, 1, 0), (0, 0, 0))),\n",
    "      Conv(16, (3, 3)),\n",
    "      BatchNorm(), Relu,\n",
    "      ConvBlock(3, [16, 16], strides=(1, 1)),\n",
    "      IdentityBlock(3, [16, 16]),\n",
    "      IdentityBlock(3, [16, 16]),\n",
    "      ConvBlock(3, [32, 32]),\n",
    "      IdentityBlock(3, [32, 32]),\n",
    "      IdentityBlock(3, [32, 32]),\n",
    "      ConvBlock(3, [64, 64]),\n",
    "      IdentityBlock(3, [64, 64]),\n",
    "      IdentityBlock(3, [64, 64]),\n",
    "      AvgPool((8, 8)), Flatten, Dense(num_classes), LogSoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(logits, labels):\n",
    "  return -np.mean(np.sum(logits * labels, axis=-1))\n",
    "\n",
    "\n",
    "def loss_fun(params, batch, predict_fun, rng=None):\n",
    "  inputs, labels = batch\n",
    "  logits = predict_fun(params, inputs, rng=rng)\n",
    "  return cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "  predicted_class = np.argmax(logits, axis=1)\n",
    "  labels_class = np.argmax(labels, axis=1)\n",
    "  return np.mean(predicted_class == labels_class)\n",
    "\n",
    "\n",
    "def jit_update_fun(model_fun, loss, opt):\n",
    "  opt_update, get_params = opt\n",
    "  def update(i, opt_state, batch, rng=None):\n",
    "    params = get_params(opt_state)\n",
    "    grads = grad(loss_fun)(params, batch, model_fun, rng)\n",
    "    return opt_update(i, grads, opt_state)\n",
    "  return jit(update)\n",
    "\n",
    "\n",
    "def jit_predict_fun(model_fun):\n",
    "  def predict(params, inputs, rng=None):\n",
    "    return jit(model_fun)(params, inputs, rng=rng)\n",
    "  return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_ds_batches('cifar10', data_dir, 10, 'train', batch_size)\n",
    "_, train_len, img_shape, num_classes = train_ds\n",
    "test_ds = get_ds_batches('cifar10', data_dir, 10, 'test', batch_size)\n",
    "_, test_len, _, _ = test_ds\n",
    "input_shape = (batch_size,) + img_shape\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.momentum(step_size, mass=0.9)\n",
    "\n",
    "init_fun, predict_fun = ResNet20(10)\n",
    "_, init_params = init_fun(key, input_shape)\n",
    "opt_state = opt_init(init_params)\n",
    "\n",
    "update_step = jit_update_fun(predict_fun, loss_fun, (opt_update, get_params))\n",
    "predict_step = jit_predict_fun(predict_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c173d2cd54e147caa9929217bedc3c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0,  Time/Step: 52.984s  Loss: 3.00426  Acc: 0.094\n",
      "Step: 100,  Time/Step: 0.536s  Loss: 2.68707  Acc: 0.125\n",
      "Step: 200,  Time/Step: 0.396s  Loss: 2.42020  Acc: 0.188\n",
      "Step: 300,  Time/Step: 0.350s  Loss: 2.23849  Acc: 0.156\n",
      "Step: 400,  Time/Step: 0.320s  Loss: 2.05773  Acc: 0.188\n",
      "Step: 500,  Time/Step: 0.302s  Loss: 2.13917  Acc: 0.188\n",
      "Step: 600,  Time/Step: 0.290s  Loss: 1.96442  Acc: 0.312\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for ep in range(num_epochs):\n",
    "  train_batches, _, _, _ = get_ds_batches('cifar10', data_dir, 10, 'train', batch_size)\n",
    "  start_time = time.time()\n",
    "  for i, batch in tqdm_notebook(enumerate(train_batches), total=train_len):\n",
    "    opt_state = update_step(step, opt_state, batch)\n",
    "    if step % 100 == 0:\n",
    "      inputs, labels = batch\n",
    "      logits = predict_step(get_params(opt_state), inputs)\n",
    "      print(f'Step: {step:d},  '\n",
    "            f'Time/Step: {(time.time() - start_time) / (i + 1):.3f}s  '\n",
    "            f'Loss: {cross_entropy(logits, labels):.5f}  '\n",
    "            f'Acc: {accuracy(logits, labels):.3f}')\n",
    "    step += 1\n",
    "  trained_params = get_params(opt_state)\n",
    "  \n",
    "  test_metrics = collections.defaultdict(float)\n",
    "  test_batches, _, _, _ = get_ds_batches('cifar10', data_dir, 10, 'test', batch_size)\n",
    "  start_time = time.time()\n",
    "  for i, batch in tqdm_notebook(enumerate(test_batches), total=test_len):\n",
    "    inputs, labels = batch\n",
    "    logits = predict_step(trained_params, inputs)\n",
    "    test_metrics['loss'] += cross_entropy(logits, labels)\n",
    "    test_metrics['acc'] += accuracy(logits, labels)\n",
    "  \n",
    "  print(f'Epoch: {ep:d}  '\n",
    "        f'Time/Step: {(time.time() - start_time) / (i + 1):.3f}s  '\n",
    "        f\"Eval Loss: {test_metrics['loss'] / test_len:.5f}  \"\n",
    "        f\"Eval Acc: {test_metrics['acc'] / test_len:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
